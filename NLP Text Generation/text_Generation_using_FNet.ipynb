{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text Generation using FNet.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zMYuj6AVAPoe"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from keras import layers\n",
        "import keras\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re"
      ],
      "metadata": {
        "id": "HXkaL2ROAWcx"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining hyperparameters\n",
        "\n",
        "VOCAB_SIZE = 8192\n",
        "MAX_SAMPLES = 50000\n",
        "BUFFER_SIZE = 20000\n",
        "MAX_LENGTH = 40\n",
        "EMBED_DIM = 256\n",
        "LATENT_DIM = 512\n",
        "NUM_HEADS = 8\n",
        "BATCH_SIZE = 64"
      ],
      "metadata": {
        "id": "9rBKu-87FLgm"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_zip = tf.keras.utils.get_file(\"cornell_movie_dialogs.zip\",\n",
        "    origin=\"http://www.cs.cornell.edu/~cristian/data/cornell_movie_dialogs_corpus.zip\",\n",
        "    extract=True,\n",
        ")\n",
        "\n",
        "path_to_dataset = os.path.join(\n",
        "    os.path.dirname(path_to_zip), \"cornell movie-dialogs corpus\"\n",
        ")\n",
        "path_to_movie_lines = os.path.join(path_to_dataset, \"movie_lines.txt\")\n",
        "path_to_movie_conversations = os.path.join(path_to_dataset, \"movie_conversations.txt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61exApIEAZ5G",
        "outputId": "6631c5a2-1c22-4bcd-b719-c4c7cfe5cb0a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from http://www.cs.cornell.edu/~cristian/data/cornell_movie_dialogs_corpus.zip\n",
            "9920512/9916637 [==============================] - 1s 0us/step\n",
            "9928704/9916637 [==============================] - 1s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_movie_conversations[12:]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "NgWDTn08CCFp",
        "outputId": "51774d76-19d0-4a6c-bab3-442f0fe42de6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/datasets/cornell movie-dialogs corpus/movie_conversations.txt'"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_conversations():\n",
        "    # Helper function for loading the conversation splits\n",
        "    id2line = {}\n",
        "    with open(path_to_movie_lines, errors=\"ignore\") as file:\n",
        "        lines = file.readlines()\n",
        "    for line in lines:\n",
        "        parts = line.replace(\"\\n\", \"\").split(\" +++$+++ \")\n",
        "        id2line[parts[0]] = parts[4]\n",
        "\n",
        "    inputs, outputs = [], []\n",
        "    with open(path_to_movie_conversations, \"r\") as file:\n",
        "        lines = file.readlines()\n",
        "    for line in lines:\n",
        "        parts = line.replace(\"\\n\", \"\").split(\" +++$+++ \")\n",
        "        # get conversation in a list of line ID\n",
        "        conversation = [line[1:-1] for line in parts[3][1:-1].split(\", \")]\n",
        "        for i in range(len(conversation) - 1):\n",
        "            inputs.append(id2line[conversation[i]])\n",
        "            outputs.append(id2line[conversation[i + 1]])\n",
        "            if len(inputs) >= MAX_SAMPLES:\n",
        "                return inputs, outputs\n",
        "    return inputs, outputs\n",
        "\n",
        "\n",
        "questions, answers = load_conversations()\n",
        "\n",
        "# Splitting training and validation sets\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((questions[:40000], answers[:40000]))\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((questions[40000:], answers[40000:]))"
      ],
      "metadata": {
        "id": "5KAUQKYYDilO"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Preprocessing and Tokenization\n",
        "def preprocess_text(sentence):\n",
        "    sentence = tf.strings.lower(sentence)\n",
        "    # Adding a space between the punctuation and the last word to allow better tokenization\n",
        "    sentence = tf.strings.regex_replace(sentence, r\"([?.!,])\", r\" \\1 \")\n",
        "    # Replacing multiple continuous spaces with a single space\n",
        "    sentence = tf.strings.regex_replace(sentence, r\"\\s\\s+\", \" \")\n",
        "    # Replacing non english words with spaces\n",
        "    sentence = tf.strings.regex_replace(sentence, r\"[^a-z?.!,]+\", \" \")\n",
        "    sentence = tf.strings.strip(sentence)\n",
        "    sentence = tf.strings.join([\"[start]\", sentence, \"[end]\"], separator=\" \")\n",
        "    return sentence\n",
        "\n",
        "\n",
        "vectorizer = layers.TextVectorization(\n",
        "    VOCAB_SIZE,\n",
        "    standardize=preprocess_text,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=MAX_LENGTH,\n",
        ")\n",
        "\n",
        "# We will adapt the vectorizer to both the questions and answers\n",
        "# This dataset is batched to parallelize and speed up the process\n",
        "vectorizer.adapt(tf.data.Dataset.from_tensor_slices((questions + answers)).batch(128))"
      ],
      "metadata": {
        "id": "D1Pe4u4GEfoG"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenizing and padding sentences using TextVectorization\n",
        "def vectorize_text(inputs, outputs):\n",
        "    inputs, outputs = vectorizer(inputs), vectorizer(outputs)\n",
        "    # One extra padding token to the right to match the output shape\n",
        "    outputs = tf.pad(outputs, [[0, 1]])\n",
        "    return (\n",
        "        {\"encoder_inputs\": inputs, \"decoder_inputs\": outputs[:-1]},\n",
        "        {\"outputs\": outputs[1:]},\n",
        "    )\n",
        "\n",
        "\n",
        "train_dataset = train_dataset.map(vectorize_text, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "val_dataset = val_dataset.map(vectorize_text, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "train_dataset = (\n",
        "    train_dataset.cache()\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE)\n",
        "    .prefetch(tf.data.AUTOTUNE)\n",
        ")\n",
        "val_dataset = val_dataset.cache().batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "HGmYZeRAFi6q"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating the FNet Encoder\n",
        "\n",
        "class FNetEncoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, **kwargs):\n",
        "        super(FNetEncoder, self).__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [\n",
        "                layers.Dense(dense_dim, activation=\"relu\"),\n",
        "                layers.Dense(embed_dim),\n",
        "            ]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Casting the inputs to complex64\n",
        "        inp_complex = tf.cast(inputs, tf.complex64)\n",
        "        # Projecting the inputs to the frequency domain using FFT2D and\n",
        "        # extracting the real part of the output\n",
        "        fft = tf.math.real(tf.signal.fft2d(inp_complex))\n",
        "        proj_input = self.layernorm_1(inputs + fft)\n",
        "        proj_output = self.dense_proj(proj_input)\n",
        "        return self.layernorm_2(proj_input + proj_output)\n"
      ],
      "metadata": {
        "id": "ECzNRttVG4PE"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating a decoder\n",
        "\n",
        "class PositionalEmbedding(layers.Layer):\n",
        "    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n",
        "        super(PositionalEmbedding, self).__init__(**kwargs)\n",
        "        self.token_embeddings = layers.Embedding(\n",
        "            input_dim=vocab_size, output_dim=embed_dim\n",
        "        )\n",
        "        self.position_embeddings = layers.Embedding(\n",
        "            input_dim=sequence_length, output_dim=embed_dim\n",
        "        )\n",
        "        self.sequence_length = sequence_length\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "    def call(self, inputs):\n",
        "        length = tf.shape(inputs)[-1]\n",
        "        positions = tf.range(start=0, limit=length, delta=1)\n",
        "        embedded_tokens = self.token_embeddings(inputs)\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        return embedded_tokens + embedded_positions\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return tf.math.not_equal(inputs, 0)\n",
        "\n",
        "\n",
        "class FNetDecoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, latent_dim, num_heads, **kwargs):\n",
        "        super(FNetDecoder, self).__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.latent_dim = latent_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention_1 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim\n",
        "        )\n",
        "        self.attention_2 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim\n",
        "        )\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [\n",
        "                layers.Dense(latent_dim, activation=\"relu\"),\n",
        "                layers.Dense(embed_dim),\n",
        "            ]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "        self.layernorm_3 = layers.LayerNormalization()\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def call(self, inputs, encoder_outputs, mask=None):\n",
        "        causal_mask = self.get_causal_attention_mask(inputs)\n",
        "        if mask is not None:\n",
        "            padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype=\"int32\")\n",
        "            padding_mask = tf.minimum(padding_mask, causal_mask)\n",
        "\n",
        "        attention_output_1 = self.attention_1(\n",
        "            query=inputs, value=inputs, key=inputs, attention_mask=causal_mask\n",
        "        )\n",
        "        out_1 = self.layernorm_1(inputs + attention_output_1)\n",
        "\n",
        "        attention_output_2 = self.attention_2(\n",
        "            query=out_1,\n",
        "            value=encoder_outputs,\n",
        "            key=encoder_outputs,\n",
        "            attention_mask=padding_mask,\n",
        "        )\n",
        "        out_2 = self.layernorm_2(out_1 + attention_output_2)\n",
        "\n",
        "        proj_output = self.dense_proj(out_2)\n",
        "        return self.layernorm_3(out_2 + proj_output)\n",
        "\n",
        "    def get_causal_attention_mask(self, inputs):\n",
        "        input_shape = tf.shape(inputs)\n",
        "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
        "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
        "        j = tf.range(sequence_length)\n",
        "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
        "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
        "        mult = tf.concat(\n",
        "            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],\n",
        "            axis=0,\n",
        "        )\n",
        "        return tf.tile(mask, mult)\n",
        "\n",
        "\n",
        "def create_model():\n",
        "    encoder_inputs = keras.Input(shape=(None,), dtype=\"int32\", name=\"encoder_inputs\")\n",
        "    x = PositionalEmbedding(MAX_LENGTH, VOCAB_SIZE, EMBED_DIM)(encoder_inputs)\n",
        "    encoder_outputs = FNetEncoder(EMBED_DIM, LATENT_DIM)(x)\n",
        "    encoder = keras.Model(encoder_inputs, encoder_outputs)\n",
        "    decoder_inputs = keras.Input(shape=(None,), dtype=\"int32\", name=\"decoder_inputs\")\n",
        "    encoded_seq_inputs = keras.Input(\n",
        "        shape=(None, EMBED_DIM), name=\"decoder_state_inputs\"\n",
        "    )\n",
        "    x = PositionalEmbedding(MAX_LENGTH, VOCAB_SIZE, EMBED_DIM)(decoder_inputs)\n",
        "    x = FNetDecoder(EMBED_DIM, LATENT_DIM, NUM_HEADS)(x, encoded_seq_inputs)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    decoder_outputs = layers.Dense(VOCAB_SIZE, activation=\"softmax\")(x)\n",
        "    decoder = keras.Model(\n",
        "        [decoder_inputs, encoded_seq_inputs], decoder_outputs, name=\"outputs\"\n",
        "    )\n",
        "    decoder_outputs = decoder([decoder_inputs, encoder_outputs])\n",
        "    fnet = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs, name=\"fnet\")\n",
        "    return fnet"
      ],
      "metadata": {
        "id": "sOTg41O4HbtM"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating and training the model\n",
        "fnet = create_model()\n",
        "fnet.compile(\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "1gj7t9W9HvQW"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fnet.fit(train_dataset, epochs=30, validation_data=val_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34gTeRhDH0YH",
        "outputId": "498ff880-46d0-431d-a696-1aba8917b380"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "625/625 [==============================] - 53s 69ms/step - loss: 1.6298 - accuracy: 0.2791 - val_loss: 1.4512 - val_accuracy: 0.3157\n",
            "Epoch 2/30\n",
            "625/625 [==============================] - 43s 68ms/step - loss: 1.4612 - accuracy: 0.3121 - val_loss: 1.4121 - val_accuracy: 0.3272\n",
            "Epoch 3/30\n",
            "625/625 [==============================] - 43s 69ms/step - loss: 1.4028 - accuracy: 0.3248 - val_loss: 1.4004 - val_accuracy: 0.3315\n",
            "Epoch 4/30\n",
            "625/625 [==============================] - 43s 69ms/step - loss: 1.3611 - accuracy: 0.3347 - val_loss: 1.3936 - val_accuracy: 0.3348\n",
            "Epoch 5/30\n",
            "625/625 [==============================] - 43s 69ms/step - loss: 1.3252 - accuracy: 0.3431 - val_loss: 1.3963 - val_accuracy: 0.3348\n",
            "Epoch 6/30\n",
            "625/625 [==============================] - 43s 69ms/step - loss: 1.2916 - accuracy: 0.3513 - val_loss: 1.4029 - val_accuracy: 0.3333\n",
            "Epoch 7/30\n",
            "625/625 [==============================] - 43s 69ms/step - loss: 1.2587 - accuracy: 0.3604 - val_loss: 1.4082 - val_accuracy: 0.3307\n",
            "Epoch 8/30\n",
            "625/625 [==============================] - 43s 69ms/step - loss: 1.2251 - accuracy: 0.3701 - val_loss: 1.4263 - val_accuracy: 0.3277\n",
            "Epoch 9/30\n",
            "625/625 [==============================] - 43s 69ms/step - loss: 1.1915 - accuracy: 0.3805 - val_loss: 1.4383 - val_accuracy: 0.3279\n",
            "Epoch 10/30\n",
            "625/625 [==============================] - 43s 69ms/step - loss: 1.1582 - accuracy: 0.3919 - val_loss: 1.4648 - val_accuracy: 0.3219\n",
            "Epoch 11/30\n",
            "625/625 [==============================] - 43s 69ms/step - loss: 1.1243 - accuracy: 0.4034 - val_loss: 1.4857 - val_accuracy: 0.3204\n",
            "Epoch 12/30\n",
            "625/625 [==============================] - 43s 69ms/step - loss: 1.0896 - accuracy: 0.4164 - val_loss: 1.5127 - val_accuracy: 0.3159\n",
            "Epoch 13/30\n",
            "625/625 [==============================] - 43s 69ms/step - loss: 1.0576 - accuracy: 0.4280 - val_loss: 1.5403 - val_accuracy: 0.3140\n",
            "Epoch 14/30\n",
            "625/625 [==============================] - 43s 69ms/step - loss: 1.0256 - accuracy: 0.4410 - val_loss: 1.5779 - val_accuracy: 0.3124\n",
            "Epoch 15/30\n",
            "625/625 [==============================] - 43s 69ms/step - loss: 0.9940 - accuracy: 0.4535 - val_loss: 1.6165 - val_accuracy: 0.3047\n",
            "Epoch 16/30\n",
            "625/625 [==============================] - 43s 69ms/step - loss: 0.9644 - accuracy: 0.4656 - val_loss: 1.6446 - val_accuracy: 0.3024\n",
            "Epoch 17/30\n",
            "625/625 [==============================] - 43s 69ms/step - loss: 0.9356 - accuracy: 0.4777 - val_loss: 1.6667 - val_accuracy: 0.2991\n",
            "Epoch 18/30\n",
            "625/625 [==============================] - 43s 69ms/step - loss: 0.9079 - accuracy: 0.4898 - val_loss: 1.7165 - val_accuracy: 0.2974\n",
            "Epoch 19/30\n",
            "625/625 [==============================] - 43s 69ms/step - loss: 0.8816 - accuracy: 0.5011 - val_loss: 1.7700 - val_accuracy: 0.2952\n",
            "Epoch 20/30\n",
            "625/625 [==============================] - 43s 69ms/step - loss: 0.8559 - accuracy: 0.5124 - val_loss: 1.7873 - val_accuracy: 0.2889\n",
            "Epoch 21/30\n",
            "625/625 [==============================] - 43s 69ms/step - loss: 0.8328 - accuracy: 0.5230 - val_loss: 1.8080 - val_accuracy: 0.2904\n",
            "Epoch 22/30\n",
            "625/625 [==============================] - 43s 69ms/step - loss: 0.8105 - accuracy: 0.5333 - val_loss: 1.8494 - val_accuracy: 0.2865\n",
            "Epoch 23/30\n",
            "625/625 [==============================] - 43s 69ms/step - loss: 0.7883 - accuracy: 0.5433 - val_loss: 1.8822 - val_accuracy: 0.2885\n",
            "Epoch 24/30\n",
            "625/625 [==============================] - 43s 69ms/step - loss: 0.7682 - accuracy: 0.5526 - val_loss: 1.9029 - val_accuracy: 0.2808\n",
            "Epoch 25/30\n",
            "625/625 [==============================] - 43s 69ms/step - loss: 0.7492 - accuracy: 0.5610 - val_loss: 1.9199 - val_accuracy: 0.2845\n",
            "Epoch 26/30\n",
            "625/625 [==============================] - 43s 69ms/step - loss: 0.7319 - accuracy: 0.5702 - val_loss: 1.9773 - val_accuracy: 0.2788\n",
            "Epoch 27/30\n",
            "625/625 [==============================] - 43s 69ms/step - loss: 0.7133 - accuracy: 0.5785 - val_loss: 2.0225 - val_accuracy: 0.2787\n",
            "Epoch 28/30\n",
            "625/625 [==============================] - 43s 69ms/step - loss: 0.6968 - accuracy: 0.5872 - val_loss: 2.0346 - val_accuracy: 0.2773\n",
            "Epoch 29/30\n",
            "625/625 [==============================] - 43s 69ms/step - loss: 0.6817 - accuracy: 0.5941 - val_loss: 2.0859 - val_accuracy: 0.2741\n",
            "Epoch 30/30\n",
            "625/625 [==============================] - 43s 69ms/step - loss: 0.6674 - accuracy: 0.6022 - val_loss: 2.1153 - val_accuracy: 0.2759\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f0279e3f890>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# serialize model to JSON\n",
        "model_json = fnet.to_json()\n",
        "with open(\"model.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "# serialize weights to HDF5\n",
        "model.save_weights(\"model.h5\")\n",
        "print(\"Saved model to disk\")"
      ],
      "metadata": {
        "id": "Od1XdKE-3_Pn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 774
        },
        "collapsed": true,
        "outputId": "9d8d4b58-f3b5-4599-d674-b9629ff99b52"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  layer_config = serialize_layer_fn(layer)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NotImplementedError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-409f30e8a69f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# serialize model to JSON\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel_json\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"model.json\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mjson_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mjson_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_json\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# serialize weights to HDF5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mto_json\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m   2660\u001b[0m         \u001b[0mA\u001b[0m \u001b[0mJSON\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2661\u001b[0m     \"\"\"\n\u001b[0;32m-> 2662\u001b[0;31m     \u001b[0mmodel_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_updated_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2663\u001b[0m     return json.dumps(\n\u001b[1;32m   2664\u001b[0m         model_config, default=json_utils.get_json_type, **kwargs)\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_updated_config\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2618\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mkeras_version\u001b[0m  \u001b[0;31m# pylint: disable=g-import-not-at-top\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2620\u001b[0;31m     \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2621\u001b[0m     model_config = {\n\u001b[1;32m   2622\u001b[0m         \u001b[0;34m'class_name'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py\u001b[0m in \u001b[0;36mget_config\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    683\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_network_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py\u001b[0m in \u001b[0;36mget_network_config\u001b[0;34m(network, serialize_layer_fn)\u001b[0m\n\u001b[1;32m   1408\u001b[0m           \u001b[0mfiltered_inbound_nodes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1410\u001b[0;31m       \u001b[0mlayer_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mserialize_layer_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1411\u001b[0m       \u001b[0mlayer_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1412\u001b[0m       \u001b[0mlayer_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'inbound_nodes'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiltered_inbound_nodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mserialize_keras_object\u001b[0;34m(instance)\u001b[0m\n\u001b[1;32m    509\u001b[0m         return serialize_keras_class_and_config(\n\u001b[1;32m    510\u001b[0m             name, {_LAYER_UNDEFINED_CONFIG_KEY: True})\n\u001b[0;32m--> 511\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    512\u001b[0m     \u001b[0mserialization_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mserialize_keras_object\u001b[0;34m(instance)\u001b[0m\n\u001b[1;32m    504\u001b[0m     \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_registered_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 506\u001b[0;31m       \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    507\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mNotImplementedError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0m_SKIP_FAILED_SERIALIZATION\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36mget_config\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    774\u001b[0m                       \u001b[0;34m\"arg2\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marg2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    775\u001b[0m                   }})\n\u001b[0;32m--> 776\u001b[0;31m                   return config\"\"\"))\n\u001b[0m\u001b[1;32m    777\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotImplementedError\u001b[0m: \nLayer PositionalEmbedding has arguments ['self', 'sequence_length', 'vocab_size', 'embed_dim']\nin `__init__` and therefore must override `get_config()`.\n\nExample:\n\nclass CustomLayer(keras.layers.Layer):\n    def __init__(self, arg1, arg2):\n        super().__init__()\n        self.arg1 = arg1\n        self.arg2 = arg2\n\n    def get_config(self):\n        config = super().get_config()\n        config.update({\n            \"arg1\": self.arg1,\n            \"arg2\": self.arg2,\n        })\n        return config"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#performing inference\n",
        "VOCAB = vectorizer.get_vocabulary()\n",
        "\n",
        "\n",
        "def decode_sentence(input_sentence):\n",
        "    # Mapping the input sentence to tokens and adding start and end tokens\n",
        "    tokenized_input_sentence = vectorizer(\n",
        "        tf.constant(\"[start] \" + preprocess_text(input_sentence) + \" [end]\")\n",
        "    )\n",
        "    # Initializing the initial sentence consisting of only the start token.\n",
        "    tokenized_target_sentence = tf.expand_dims(VOCAB.index(\"[start]\"), 0)\n",
        "    decoded_sentence = \"\"\n",
        "\n",
        "    for i in range(MAX_LENGTH):\n",
        "        # Get the predictions\n",
        "        predictions = fnet.predict(\n",
        "            {\n",
        "                \"encoder_inputs\": tf.expand_dims(tokenized_input_sentence, 0),\n",
        "                \"decoder_inputs\": tf.expand_dims(\n",
        "                    tf.pad(\n",
        "                        tokenized_target_sentence,\n",
        "                        [[0, MAX_LENGTH - tf.shape(tokenized_target_sentence)[0]]],\n",
        "                    ),\n",
        "                    0,\n",
        "                ),\n",
        "            }\n",
        "        )\n",
        "        # Calculating the token with maximum probability and getting the corresponding word\n",
        "        sampled_token_index = tf.argmax(predictions[0, i, :])\n",
        "        sampled_token = VOCAB[sampled_token_index.numpy()]\n",
        "        # If sampled token is the end token then stop generating and return the sentence\n",
        "        if tf.equal(sampled_token_index, VOCAB.index(\"[end]\")):\n",
        "            break\n",
        "        decoded_sentence += sampled_token + \" \"\n",
        "        tokenized_target_sentence = tf.concat(\n",
        "            [tokenized_target_sentence, [sampled_token_index]], 0\n",
        "        )\n",
        "\n",
        "    return decoded_sentence\n",
        "\n",
        "\n",
        "decode_sentence(\"I have to be home in twenty minutes.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "433u1RCUH-cW",
        "outputId": "026bf839-45aa-4fe3-b3ff-d08784596c06"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'i m sorry . i m sorry . i m struggling here . you also any part of it . you re all your life . you re finding no rich . you re gonna make me the difference  '"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "sIpw9xVuJph_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}